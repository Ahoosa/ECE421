{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "a1 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3p_v1xnlvfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a7efc604-01cb-46e9-f89f-caa6e7a9d5ac"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o29MgwFRsZMe"
      },
      "source": [
        "def loadData():\n",
        "    with np.load('notMNIST.npz') as dataset:\n",
        "        Data, Target = dataset['images'], dataset['labels']\n",
        "        posClass = 2\n",
        "        negClass = 9\n",
        "        dataIndx = (Target==posClass) + (Target==negClass)\n",
        "        Data = Data[dataIndx]/255.\n",
        "        Target = Target[dataIndx].reshape(-1, 1)\n",
        "        Target[Target==posClass] = 1\n",
        "        Target[Target==negClass] = 0\n",
        "        np.random.seed(421)\n",
        "        randIndx = np.arange(len(Data))\n",
        "        np.random.shuffle(randIndx)\n",
        "        Data, Target = Data[randIndx], Target[randIndx]\n",
        "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
        "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
        "        testData, testTarget = Data[3600:], Target[3600:]\n",
        "    return trainData, validData, testData, trainTarget, validTarget, testTarget\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwQFohHVqc0q"
      },
      "source": [
        "def y_hat(z):\n",
        "  sigma = 1 / (1+np.exp(-z))\n",
        "  return sigma\n",
        "  \n",
        "def accuracy(W,x,b,y):\n",
        "  y_h = y_hat(np.matmul(x,W)+b)\n",
        "  acc = np.sum((y_h>=0.5)==y)/np.shape(y)[0] \n",
        "  return acc\n",
        "\n",
        "def CELoss(x,y,W,b,reg):\n",
        "  z = tf.matmul(x,W) + b\n",
        "  CEloss= tf.losses.sigmoid_cross_entropy(y, tf.sigmoid(z))\n",
        "  regularizer =reg*tf.nn.l2_loss(W) \n",
        "  loss = CEloss + regularizer\n",
        "  return loss\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnQfpgh1tzu6"
      },
      "source": [
        "def buildGraph(beta1=None, beta2=None, epsilon=None):\n",
        "  \n",
        "  minibatch_size = 700\n",
        "  alpha = 0.001\n",
        "  W = tf.Variable(tf.random.truncated_normal(shape=(784, 1), mean=0.0, stddev=0.5, dtype=tf.float32,seed= None, name=\"W\"))\n",
        "  b = tf.Variable(tf.zeros(1),name=\"b\")\n",
        "  reg = 0\n",
        "  x = tf.placeholder(tf.float32, (None, 784),name = \"x\")\n",
        "  y = tf.placeholder(tf.float32, (None, 1),name = \"y\")\n",
        "  \n",
        "  valid_data = tf.placeholder(tf.float32, shape=(100, 784), name = \"valid_data\")\n",
        "  valid_target = tf.placeholder(tf.int8, shape=(100, 1), name = \"valid_target\")\n",
        "\n",
        "  test_data = tf.placeholder(tf.float32, shape=(145, 784), name = \"test_data\")\n",
        "  test_target = tf.placeholder(tf.int8, shape=(145, 1), name=\"test_target\")\n",
        "\n",
        "  z = tf.matmul(x,W) + b\n",
        "  y_hat = tf.sigmoid(z)\n",
        "  loss = CELoss(x,y,W,b,reg)\n",
        "\n",
        "  z_valid = tf.matmul(valid_data,W) + b\n",
        "  y_hat_valid = tf.sigmoid(z_valid)\n",
        "  validLoss = CELoss(valid_data,valid_target,W,b,reg)\n",
        "\n",
        "  z_test = tf.matmul(test_data,W) + b\n",
        "  y_hat_test = tf.sigmoid(z_test)\n",
        "  testLoss = CELoss(test_data,test_target,W,b,reg)\n",
        "  \n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=alpha).minimize(loss) \n",
        "  \n",
        "  with tf.Session() as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    \n",
        "    trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
        "    validData = validData.reshape((validData.shape[0],validData.shape[1]*validData.shape[2]))\n",
        "    trainData = trainData.reshape((trainData.shape[0],trainData.shape[1]*trainData.shape[2]))\n",
        "    testData = testData.reshape((testData.shape[0],testData.shape[1]*testData.shape[2]))\n",
        "    \n",
        "    trainLossArr = []\n",
        "    validLossArr = []\n",
        "    testLossArr = []\n",
        "    trainAccuracy = []\n",
        "    validAccuracy = []\n",
        "    testAccuracy = []\n",
        "\n",
        "    # SGD implementation\n",
        "    epochs = 700\n",
        "    N = trainData.shape[0]\n",
        "   \n",
        "    # total number of batches required\n",
        "    batchRange = int(N/minibatch_size) \n",
        "    \n",
        "    for step in range(epochs):\n",
        "      #shuffling data\n",
        "      newInd = np.arange(len(trainData))\n",
        "      np.random.shuffle(newInd)\n",
        "      trainData, trainTarget = trainData[newInd], trainTarget[newInd]\n",
        "      for j in range(batchRange):  \n",
        "        #sampling           \n",
        "        XBatch = trainData[j*minibatch_size:(j+1)*minibatch_size]\n",
        "        YBatch = trainTarget[j*minibatch_size:(j+1)*minibatch_size]\n",
        "      \n",
        "        my_dict = { x: XBatch, y: YBatch, valid_data: validData, valid_target: validTarget, test_data: testData,test_target: testTarget}\n",
        "        opt, updated_w, updated_b, train_loss, pred_y, valid_loss, valid_pred, test_loss, test_pred = session.run([optimizer, W, b, loss,y_hat, validLoss,  y_hat_valid, testLoss, y_hat_test], feed_dict=my_dict)\n",
        "        \n",
        "      trainLossArr.append(train_loss)\n",
        "      trainAccuracy.append(accuracy(updated_w,trainData,updated_b,trainTarget))\n",
        "       \n",
        "      validLossArr.append(valid_loss)\n",
        "      validAccuracy.append(accuracy(updated_w,validData,updated_b,validTarget))\n",
        "        \n",
        "      testLossArr.append(test_loss)\n",
        "      testAccuracy.append(accuracy(updated_w,testData,updated_b,testTarget))\n",
        "       \n",
        "  return trainLossArr,validLossArr,testLossArr,trainAccuracy,validAccuracy,testAccuracy\n",
        "  \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81EqusieLiqf"
      },
      "source": [
        "def plot(figureNum, title,yLabel,trainArray,validArray,testArray):  \n",
        "    f = plt.figure(figureNum)\n",
        "    title = title \n",
        "    plt.title(title)  \n",
        "    plt.ylabel(yLabel)\n",
        "    plt.xlabel('Iterations')  \n",
        "    # trainArray = savgol_filter(trainArray, 101, 4)\n",
        "    plt.plot(range(700),trainArray)\n",
        "    plt.plot(range(700),validArray)  \n",
        "    # plt.plot(range(700),testArray)  \n",
        "    plt.legend([\"Training \"+yLabel,\"Valid \"+yLabel],loc='upper right')\n",
        "    plt.show()\n",
        "    # plt.savefig(str(figureNum))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpU24nvsRN6D"
      },
      "source": [
        "trainLossArr,validLossArr,testLossArr,trainAccuracy,validAccuracy,testAccuracy = buildGraph()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDT025V8s2Aq"
      },
      "source": [
        "# Run with each of the following minibatch sizes: \\\\\n",
        "# Batch = [100,700,1750] \\\\\n",
        "# Set minibatch size to 500 and adjust the parameters of Adam optimizer one by one: \\\\\n",
        "# β1 = {0.95, 0.99} \\\\\n",
        "# β2 = {0.99, 0.9999} \\\\\n",
        "# ε={1e−09,1e−4} \\\\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KBfmAxuCYRZ"
      },
      "source": [
        "plot(3, \"Training and Validation Loss with α=0.001 batch=700\",\"Loss\",trainLossArr,validLossArr,testLossArr)\n",
        "plot(4, \"Training and Validation Accuracy with α=0.001 batch=700\",\"Accuracy\",trainAccuracy,validAccuracy,testAccuracy)\n",
        "print(\"training accuracy batch 500:\")\n",
        "print(trainAccuracy[699]) \n",
        "print(\"valid accuracy batch 500:\")\n",
        "print(validAccuracy[699])\n",
        "print(\"testing accuracy batch 500:\")\n",
        "print(testAccuracy[699])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_56E9Aoci4h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}